{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Header1"
      },
      "source": [
        "# AI Query Tube\n",
        "\n",
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imports"
      },
      "outputs": [],
      "source": [
        "%pip install youtube-transcript-api sentence-transformers tf-keras\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound, VideoUnavailable\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Header2"
      },
      "source": [
        "## 2. YouTube Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApiKey"
      },
      "outputs": [],
      "source": [
        "API_KEY = os.environ.get('YOUTUBE_API_KEY')\n",
        "if API_KEY is None:\n",
        "    raise RuntimeError('Set YOUTUBE_API_KEY in the environment before running this notebook')\n",
        "CHANNEL_ID = \"UC4SVo0Ue36XCfOyb5Lh1viQ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DataCollection"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "params = {\n",
        "    \"key\": API_KEY,\n",
        "    \"channelId\": CHANNEL_ID,\n",
        "    \"part\": \"snippet,id\",\n",
        "    \"order\": \"date\",\n",
        "    \"maxResults\": 50,\n",
        "    \"hl\": \"en\",\n",
        "    \"regionCode\": \"US\"\n",
        "}\n",
        "\n",
        "def extract_video_fields(item):\n",
        "    video_id = item[\"id\"][\"videoId\"]\n",
        "    title = item[\"snippet\"][\"title\"]\n",
        "    published = item[\"snippet\"][\"publishedAt\"]\n",
        "    return video_id, title, published\n",
        "\n",
        "videos = []\n",
        "next_page_token = None\n",
        "\n",
        "# Set a limit of pages to fetch to avoid infinite loops if something goes wrong, or remove for full channel\n",
        "max_pages = 10\n",
        "page_count = 0\n",
        "\n",
        "while True:\n",
        "    if next_page_token:\n",
        "        params['pageToken'] = next_page_token\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(url, params=params, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    except Exception as e:\n",
        "        print('Request failed:', e)\n",
        "        break\n",
        "\n",
        "    for item in data.get(\"items\", []):\n",
        "        if \"videoId\" in item.get(\"id\", {}):\n",
        "            video_id, title, published = extract_video_fields(item)\n",
        "            videos.append([video_id, title, published])\n",
        "\n",
        "    next_page_token = data.get(\"nextPageToken\")\n",
        "    page_count += 1\n",
        "    if not next_page_token or page_count >= max_pages:\n",
        "        break\n",
        "\n",
        "df = pd.DataFrame(videos, columns=[\"video_id\", \"title\", \"published_date\"])\n",
        "df.to_csv(\"youtube_metadata.csv\", index=False)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Header3"
      },
      "source": [
        "## 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDA"
      },
      "outputs": [],
      "source": [
        "# Basic info\n",
        "display(df.info())\n",
        "display(df.describe())\n",
        "\n",
        "# Missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Publish date distribution\n",
        "df['published_date'] = pd.to_datetime(df['published_date'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['published_date'].dt.year.value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Video Distribution by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Videos')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Header4"
      },
      "source": [
        "## 4. Transcript Fetching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Transcripts"
      },
      "outputs": [],
      "source": [
        "# Using logic from reference notebook: week3_transcripts.ipynb\n",
        "ytt_api = YouTubeTranscriptApi()\n",
        "transcripts = []\n",
        "DELAY = 1.0\n",
        "\n",
        "for vid in df[\"video_id\"]:\n",
        "    try:\n",
        "        # Reference implementation uses .fetch(video_id)\n",
        "        transcript_object = ytt_api.fetch(vid)\n",
        "        \n",
        "        # Reference implementation joins snippets\n",
        "        text = \" \".join(\n",
        "            snippet.text for snippet in transcript_object.snippets\n",
        "        )\n",
        "        transcripts.append(text)\n",
        "        print(f\"✅ Success: {vid}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed: {vid} | {type(e).__name__}: {e}\")\n",
        "        transcripts.append(None)\n",
        "    \n",
        "    time.sleep(DELAY)\n",
        "\n",
        "df[\"transcript\"] = transcripts\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Header5"
      },
      "source": [
        "## 5. Embeddings and Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Embeddings"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "df[\"text_for_embedding\"] = df[\"title\"] + \" \" + df[\"transcript\"].fillna(\"\")\n",
        "df[\"embedding\"] = df[\"text_for_embedding\"].apply(lambda x: model.encode(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Search"
      },
      "outputs": [],
      "source": [
        "query = \"PHP tutorials\"\n",
        "query_embedding = model.encode(query)\n",
        "\n",
        "scores = cosine_similarity([query_embedding], list(df[\"embedding\"]))\n",
        "top_idx = scores[0].argsort()[-5:][::-1]  # Top 5 results\n",
        "\n",
        "results = df.iloc[top_idx][[\"title\", \"video_id\"]].copy()\n",
        "results[\"score\"] = scores[0][top_idx]\n",
        "display(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
